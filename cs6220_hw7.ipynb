{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs6220_hw7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1k9o2tdjVlQUXaJg9fFcjlwvJ35JXLuzh",
      "authorship_tag": "ABX9TyN6/V8Jn0TagtfK8E6f3Eax",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linyu3294/cs6220-data-minning-hw/blob/main/cs6220_hw7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTmxPOBRTIY"
      },
      "source": [
        "\n",
        "# **HW7** **Social Graphs, Recommendation Systems**\n",
        "\n",
        "Make sure you check the syllabus for the due date. Please use the notations adopted in class, even if the problem is stated in the book using a different notation.\n",
        "\n",
        "We are not looking for very long answers (if you find yourself writing more than one or two pages of typed text per problem, you are probably on the wrong track). Try to be concise; also keep in mind that good ideas and explanations matter more than exact details.\n",
        "\n",
        "Submit all code files Dropbox (create folder HW1 or similar name). Results can be pdf or txt files, including plots/tabels if any.\n",
        "\"Paper\" exercises: submit using Dropbox as pdf, either typed or scanned handwritten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayxwJvCKwrsF"
      },
      "source": [
        "---\n",
        "DATATSET MovieLens 100K Ratings https://grouplens.org/datasets/movielens/100k/\n",
        "\n",
        "DATATSET Netflix Prize ratings Dataset https://www.kaggle.com/netflix-inc/netflix-prize-data\n",
        "\n",
        "DATATSET Friendster Social Graph http://socialcomputing.asu.edu/datasets/Friendster\n",
        "\n",
        "DATATSET Flicker Social Graph http://networkrepository.com/soc-Flickr-ASU.php, but use the one curated in DM resources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTn6OYMg-fHB"
      },
      "source": [
        "\n",
        "---\n",
        "## **PROBLEM 1: Recommender System using Collaborative Filtering**\n",
        "Implement a Movie Recommendation System and run it on the Movie Lens Dataset (Train vs Test). Mesure performance on test set using RMSE\n",
        "\n",
        ">First you are required to compute first a user-user similarity based on ratings and movies in common\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b56-6GPTKys"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def get_user_data (path) :\n",
        "  users = dict()\n",
        "  with open(path, 'r') as file:\n",
        "      lines = file.read().splitlines()\n",
        "\n",
        "      for line in lines:\n",
        "        user_id = int(line.split('\\t') [0]) - 1\n",
        "        movie_id = int(line.split('\\t')[1]) - 1 \n",
        "        rating = int(line.split('\\t')[2])\n",
        "      \n",
        "        user_ratings = users.get(user_id, [])\n",
        "        user_ratings.append((movie_id, rating))\n",
        "        users[user_id] = user_ratings\n",
        "  return users\n",
        "\n",
        "def normalize_users_data (users):\n",
        "  total = 0\n",
        "  users_norm = dict()\n",
        "  users_avg = []   \n",
        "  users_variance = []\n",
        "  for user_id in users.keys():\n",
        "    ratings = [component[1] for component in users.get(user_id)]\n",
        "    ratings_sum = np.sum(ratings)\n",
        "    avg_rating = ratings_sum/len(ratings)\n",
        "    users_avg.append(avg_rating)\n",
        "    user_variance = (ratings_sum**2 / len(ratings) ) - avg_rating**2\n",
        "    users_variance.append(user_variance)\n",
        "    normalized_ratings = [(r[0],(r[1] - avg_rating) / user_variance) for r in users.get(user_id)]\n",
        "    users_norm[user_id] = normalized_ratings\n",
        "  return users_avg , users_variance, users_norm\n",
        "\n",
        "\n",
        "def intersection(lst1, lst2):\n",
        "    lst3 = [value for value in lst1 if value in lst2]\n",
        "    return lst3\n",
        "\n",
        "\n",
        "def get_users_similarities (users_norm) :\n",
        "  users_similarity = []\n",
        "  for u in range(len(users_norm)):\n",
        "    temp_list = []\n",
        "    for v in range(len(users_norm)):\n",
        "      u_movies = [i[0] for i in users_norm[u]]\n",
        "      v_movies = [i[0] for i in users_norm[v]]\n",
        "      u_v_common_movies = intersection (u_movies, v_movies)\n",
        "      u_common_rum = [i[1] for i in users_norm[u] if i[0] in u_v_common_movies]\n",
        "      v_common_rum = [i[1] for i in users_norm[v] if i[0] in u_v_common_movies]\n",
        "      if len(u_v_common_movies) > 0:\n",
        "        similarity = np.dot(u_common_rum, v_common_rum) / len(u_v_common_movies)\n",
        "      else :\n",
        "        similarity = 0.0000001\n",
        "      temp_list.append(similarity)\n",
        "    users_similarity.append(temp_list)\n",
        "  users_similarity = np.array(users_similarity)\n",
        "  return (users_similarity)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_H4vX4iHj4d",
        "outputId": "d91f9d10-0386-472b-fbbe-1bb48b9f1f55"
      },
      "source": [
        "movies = []\n",
        "users = dict()\n",
        "ratings = dict()\n",
        "num_of_movies = 1682\n",
        "num_of_users = 943\n",
        "\n",
        "training_path = '/content/drive/MyDrive/NEU ALIGN CS Masters/CS6220 Data Mining/Colab Notebooks/ml-100k/u1.base'\n",
        "test_path = '/content/drive/MyDrive/NEU ALIGN CS Masters/CS6220 Data Mining/Colab Notebooks/ml-100k/u1.test'\n",
        "\n",
        "\n",
        "train_users= get_user_data(training_path)\n",
        "test_users =get_user_data(test_path)\n",
        "train_users_avg, train_users_variance, train_users_norm =  normalize_users_data(train_users)\n",
        "train_users_similarity = get_users_similarities (train_users_norm)\n",
        "print (train_users_similarity.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(943, 943)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjaSYgMtO5YG"
      },
      "source": [
        "---\n",
        ">Second, make rating predictions on the test set followoing the KNN idea: a prediction (user, movie) is the weighted average of other users' rating for the movie, weighted by user-similarity to the given user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tppWD-k-9XNz"
      },
      "source": [
        "def find_other_users_with_movie_rating( users, users_avg, user, movie):\n",
        "  other_users = []\n",
        "  for key, ratings in users.items():\n",
        "    # print (ratings)\n",
        "    for component in ratings:\n",
        "      if (component[0] == movie):\n",
        "        other_users.append(key)\n",
        "  return other_users\n",
        "  \n",
        "    \n",
        "\n",
        "def predict (users_similarity, train_users_norm, users_avg, users_variance, user, movie):\n",
        "  num = 0\n",
        "  denom = 0\n",
        "\n",
        "  other_users = find_other_users_with_movie_rating(train_users_norm, users_avg, user, movie)\n",
        "  # print ('\\npredicting user', user, '\\'s rating for movie ', movie)\n",
        "  # print ('Other users who also rated move ', movie, '\\n' ,other_users , '\\n\\n')\n",
        "  for other in other_users:\n",
        "      for component in train_users_norm.get(other):\n",
        "        if (component[0] == movie):\n",
        "          other_rating = component[1]\n",
        "          num += users_similarity[user][other] * other_rating\n",
        "          denom += abs(users_similarity[user][other])\n",
        "\n",
        "  # print (num)\n",
        "  # print (denom)\n",
        "  # print (users_avg[user])\n",
        "  # print ((num)/(denom))\n",
        "  # print (users_variance[user])\n",
        "  if denom !=0:\n",
        "    return users_avg[user] + ( (num)/(denom) ) * users_variance[user]\n",
        "  else:\n",
        "    print('No other user ratings.')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dUvO7gLq20m"
      },
      "source": [
        "\n",
        "  predicted_user = 29\n",
        "\n",
        "  print ('user train data', train_users.get(predicted_user), '\\n', 'user test data', test_users.get(predicted_user))\n",
        "  for component in test_users.get(predicted_user):\n",
        "      prediction = predict (train_users_similarity,\n",
        "                            train_users_norm,\n",
        "                            train_users_avg,\n",
        "                            train_users_variance,\n",
        "                            predicted_user,\n",
        "                            component[0])\n",
        "      prediction = round(prediction)\n",
        "      print ( '\\n', 'prediction of user ', predicted_user, 'for movie ', component[0],  ' : ', prediction )\n",
        "\n",
        "      print ('actual user ', predicted_user, ' rating for movie ', component[0], ' : ',   component[1], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1YxC41_SR_"
      },
      "source": [
        "---\n",
        "## **PROBLEM 2: EXTRA CREDIT Netflix Recommendations**\n",
        "\n",
        "\n",
        "Implement a recommender system on the Netflix Prize Dataset. Use the \"probe\" set for testing. For a compettitive systems, one need to add movie content features such as actors, genres, directors, music, etc. These features are not available from Netflix, but for some movies they have been crawled by Movie Title form other websites such as IMDB (for example \"movie_details.xml\" file in DM_resources, but you can get more such features on your own.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solnvyj__vEK"
      },
      "source": [
        "---\n",
        "## **PROBLEM 3: Social Community Detection**\n",
        "\n",
        "Implement a community detection algorithm on the Flicker Graph. Use the betweenes idea on edges and the Girvan–Newman Algorithm. The original dataset graph has more than 5M edges; in DM_resources there are 4 different sub-sampled graphs with edge counts from 2K to 600K; you can use these if the original is too big.\n",
        "\n",
        "You should use a library to support graph operations (edges, vertices, paths, degrees, etc). We used igraph in python which also have builtin community detection algorithms (not allowed); these are useful as a way to evaluate communities you obtain\n",
        "\n",
        "```\n",
        "https://igraph.org/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iO9j-XOZgvG"
      },
      "source": [
        "This code works in pycharm, not sure why it's not working in google colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu3Ywr9foGEN"
      },
      "source": [
        "https://github.com/linyu3294/community-graph-knowledge-graph-qa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnxHadyIccft"
      },
      "source": [
        "import csv\n",
        "import networkx as nx\n",
        "\n",
        "_DEBUG_ = None\n",
        "\n",
        "\n",
        "def print_hi(name):\n",
        "    # Use a breakpoint in the code line below to debug your script.\n",
        "    print(f'Hi, {name}')  # Press ⌘F8 to toggle the breakpoint.\n",
        "\n",
        "\n",
        "# _DEBUG_ = 1\n",
        "\n",
        "def build_graph(graph, file, delimiter):\n",
        "    reader = csv.reader(open(file), delimiter=delimiter)\n",
        "    for line in reader:\n",
        "        if len(line) > 2:\n",
        "            if float(line[2]) != 0.0:\n",
        "                graph.add_edge(int(line[0]), int(line[1]), weight=float[line[2]])\n",
        "        else:\n",
        "            graph.add_edge(int(line[0]), int(line[1]), weight=1.0)\n",
        "\n",
        "\n",
        "def cmtyGirvanNewmanStep(G):\n",
        "    # if _DEBUG_:\n",
        "    #   # print('calling CmtyGirvanNewmanStep')\n",
        "    init_ncomp = nx.number_connected_components(G)\n",
        "    ncomp = init_ncomp\n",
        "    while ncomp <= init_ncomp:\n",
        "        bw = nx.edge_betweenness_centrality(G, weight='weight')\n",
        "        print(bw.values())\n",
        "        max_ = max(bw.values())\n",
        "        for k, v in bw.items():\n",
        "            if float(v) == max_:\n",
        "                G.remove_edge(k[0], k[1])\n",
        "        ncomp = nx.number_connected_components(G)\n",
        "\n",
        "\n",
        "def getGNModularity(G, deg_, m_):\n",
        "    new_adj_matrix = nx.adj_matrix(G)\n",
        "    new_degree = {}\n",
        "    new_degree = updateDeg(new_adj_matrix, G.nodes())\n",
        "    communities = nx.connected_components(G)\n",
        "    total_communities = nx.number_connected_components(G)\n",
        "    print('number of communities in partitioned graph : ', total_communities)\n",
        "    mod = 0\n",
        "\n",
        "    # the equation that professor went over in class\n",
        "    for c in communities:\n",
        "        intra_edges = 0\n",
        "        rand_edges = 0\n",
        "        for u in c:\n",
        "            # print('new degree ', new_degree)\n",
        "            intra_edges += new_degree[u]\n",
        "            rand_edges += deg_[u]\n",
        "        # this is the adjacy list minus a random graph\n",
        "        mod += (float(intra_edges) - float(rand_edges * rand_edges) / float(2 * m_))\n",
        "    mod = mod / (2 * m_)\n",
        "    if _DEBUG_:\n",
        "        print('partition : %f' % mod)\n",
        "    return mod\n",
        "\n",
        "\n",
        "def updateDeg(adj_matrix, nodes):\n",
        "    deg_dict = {}\n",
        "    n = len(nodes)\n",
        "    b = adj_matrix.sum(axis=1)\n",
        "    for i in range(n):\n",
        "        deg_dict[i] = b[i, 0]\n",
        "    return deg_dict\n",
        "\n",
        "\n",
        "def runGirvanNewman(G, org_degree, m_):\n",
        "    best_q = 0.0\n",
        "    Q = 0.0\n",
        "    while True:\n",
        "        cmtyGirvanNewmanStep(G)\n",
        "        Q = getGNModularity(G, org_degree, m_)\n",
        "        # print('modularity of decomposed G : %f' % Q)\n",
        "        if Q > best_q:\n",
        "            best_q = Q\n",
        "            best_communities = nx.connected_components(G)\n",
        "            # print('commnunities : %f' % best_communities)\n",
        "        if G.number_of_edges() == 0:\n",
        "            break\n",
        "\n",
        "    if best_q > 0.0:\n",
        "        print('max modularity (Q) : %f' % best_q)\n",
        "        print('graph communities', best_communities)\n",
        "    else:\n",
        "        print('max modularity (Q) : %f' % best_q)\n",
        "\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/NEU ALIGN CS Masters/CS6220 Data Mining/Colab Notebooks/Flickr_sampled_edges/edges_sampled_2K.csv'\n",
        "G = nx.Graph()\n",
        "build_graph(G, path, ',')\n",
        "\n",
        "if _DEBUG_:\n",
        "    print('G nodes', G.nodes())\n",
        "    print('G number of nodes', G.number_of_nodes())\n",
        "\n",
        "n = G.number_of_nodes()\n",
        "adj_matrix = nx.adj_matrix(G)\n",
        "\n",
        "m_ = 0.0\n",
        "\n",
        "for i in range(0, n):\n",
        "    for j in range(0, n):\n",
        "        m_ += adj_matrix[i, j]\n",
        "m_ = m_ / 2.0\n",
        "\n",
        "if _DEBUG_:\n",
        "    print('m : %f' % m_)\n",
        "\n",
        "org_degree = {}\n",
        "org_degree = updateDeg(adj_matrix, G.nodes)\n",
        "\n",
        "runGirvanNewman(G, org_degree, m_)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVAhs1LbABKr"
      },
      "source": [
        "\n",
        "---\n",
        "## **PROBLEM 4: Knowledge Base Question Answering** ##\n",
        "\n",
        "Given is knowledge graph with entities and relations, questions with starting entity and answers, and their word embedding . For each question, navigate the graph from the start entiry outwards until you find appropriate answer entities.\n",
        "- there might be multiple answers correct, use F1 to evaluate\n",
        "- utils functions (similarity, load_graphs) are given, but you can choose not to use them\n",
        "- answers are given to be used for evaluation only\n",
        "- your strategy should be a graph traversal augmented with scoring of paths; you might discard paths not promising along the way. This is similar to a focused crawl strategy.\n",
        "- for simplicity, the questions are picked so that the answer is always at the end of the relevant path (not intermediary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TE2DeTqoBx3"
      },
      "source": [
        "https://github.com/linyu3294/community-graph-knowledge-graph-qa"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}